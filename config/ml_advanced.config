# =============================================================================
# Advanced ML Configuration
# =============================================================================
# Configures advanced ML components:
# - Neural Linear Algebra
# - Uncertainty Quantification
# - Inverse Problem Acceleration
# - Multi-Fidelity Learning
# - Neural Time Stepping
# - Neural AMR
# =============================================================================

[general]
enabled = true
model_directory = ml_models/advanced
use_gpu = true

# =============================================================================
# Neural Linear Algebra
# =============================================================================

[neural_coarse_grid]
enabled = false
model_path = ml_models/advanced/coarse_grid_model.pt

# Dimensions
fine_dim = 1000
coarse_dim = 100

# Networks
restriction_layers = 256,256
coarse_solve_layers = 256,512,256
prolongation_layers = 256,256
activation = gelu

# Hybrid mode
use_geometric_restriction = false
use_exact_coarse_solve = false

# Training
learning_rate = 1e-4
epochs = 200
batch_size = 32

[neural_amg]
enabled = false
model_path = ml_models/advanced/amg_model.pt

# Coarsening strategy
# Options: STRENGTH_BASED, NEURAL_STRENGTH, NEURAL_CLUSTERING, GRAPH_PARTITIONING
strategy = NEURAL_STRENGTH

# Network
hidden_dim = 64
num_layers = 3

# Training
learning_rate = 1e-3
epochs = 100

# =============================================================================
# Bayesian Neural Networks / Uncertainty Quantification
# =============================================================================

[bayesian_nn]
enabled = false
model_path = ml_models/advanced/bnn_model.pt

# Architecture
layer_sizes = 64,128,128,64
activation = relu

# Prior
prior_std = 1.0

# Training
learning_rate = 1e-3
epochs = 200
batch_size = 32
kl_weight = 1e-3

# Inference
num_samples = 100

[deep_ensemble]
enabled = true
model_path = ml_models/advanced/ensemble_model.pt

# Ensemble
num_models = 5

# Individual model
layer_sizes = 128,256,128
activation = relu

# Training
learning_rate = 1e-3
epochs = 100

# Diversity
use_different_seeds = true
use_bootstrap = false
data_fraction = 1.0

[probabilistic_fno]
enabled = false
model_path = ml_models/advanced/prob_fno_model.pt

# UQ method
# Options: BAYESIAN_WEIGHTS, ENSEMBLE, MC_DROPOUT, DEEP_EVIDENTIAL
uq_method = ENSEMBLE
num_ensemble_members = 5

# For Bayesian
prior_std = 1.0
kl_weight = 1e-4

# For MC Dropout
dropout_rate = 0.1
num_mc_samples = 50

# =============================================================================
# Neural Inversion / History Matching
# =============================================================================

[neural_forward_model]
enabled = true
model_path = ml_models/advanced/forward_model.pt

# Dimensions
num_parameters = 100
num_observations = 50

# Architecture
hidden_layers = 256,512,512,256
activation = gelu

# Time-dependent
time_dependent = false
num_time_steps = 10

# Training
learning_rate = 1e-3
epochs = 500
batch_size = 64

# Data augmentation
noise_level = 0.01
use_latin_hypercube = true
num_training_samples = 1000

[neural_enkf]
enabled = false
model_path = ml_models/advanced/enkf_model.pt

# Ensemble
ensemble_size = 100
parameter_dim = 100
observation_dim = 50

# Neural acceleration
use_neural_forward = true
use_neural_localization = true
use_neural_inflation = true

# Localization
localization_radius = 0.5

# Inflation
inflation_factor = 1.05
adaptive_inflation = true

# Training
epochs = 100

[physics_guided_inversion]
enabled = false
model_path = ml_models/advanced/physics_inversion_model.pt

# Dimensions
parameter_dim = 100
state_dim = 1000
observation_dim = 50

# Network
encoder_layers = 256,256
physics_layers = 256,512,256

# Physics constraints
# Options: darcy, wave, heat, elastic
pde_type = darcy
physics_weight = 0.1

# Regularization
l2_regularization = 1e-4
total_variation = 0.0

# Optimization
learning_rate = 1e-3
max_iterations = 1000
convergence_tol = 1e-6

# =============================================================================
# Multi-Fidelity Learning
# =============================================================================

[multi_fidelity]
enabled = false
model_path = ml_models/advanced/multi_fidelity_model.pt

# Method
# Options: ADDITIVE, MULTIPLICATIVE, RESIDUAL, TRANSFER, COKRIGING, NEURAL_OPERATOR
method = ADDITIVE

# Correction network
correction_layers = 256,256
activation = gelu

# Additive correction options
include_lf_output = true
include_x = true
include_gradient = false

# Regularization
l2_reg = 1e-4
penalize_large_correction = true
correction_penalty = 0.1

# Training
learning_rate = 1e-3
epochs = 200
batch_size = 32

# Adaptive selection
use_adaptive_selection = true
confidence_threshold = 0.9

# Active learning
use_active_learning = false
active_learning_budget = 100.0

# =============================================================================
# Neural Time Stepping
# =============================================================================

[neural_timestep]
enabled = true
model_path = ml_models/advanced/timestep_model.pt

# Feature extraction
state_history_length = 5
use_gradient_features = true
use_curvature_features = true
use_physics_features = true

# Constraints
dt_min = 1e-10
dt_max = 1e3
max_change_ratio = 2.0

# Network
hidden_layers = 128,128
activation = relu

# Training
learning_rate = 1e-3
epochs = 200

# Safety
safety_factor = 0.9

# Online learning
enable_online_learning = true

[neural_imex]
enabled = true
model_path = ml_models/advanced/imex_model.pt

# Transition triggers (classical thresholds)
stress_threshold = 0.8
slip_rate_threshold = 1e-3
velocity_threshold = 1e-4

# Network
hidden_layers = 128,256,128

# Lookahead prediction
use_lookahead = true
lookahead_steps = 5

# Training
learning_rate = 1e-3
epochs = 100

# =============================================================================
# Neural AMR
# =============================================================================

[neural_amr]
enabled = true
model_path = ml_models/advanced/amr_model.pt

# Component selection
use_neural_error = true
use_predictive = true
use_physics_aware = true
use_feature_tracking = true

# Refinement parameters
refinement_threshold = 0.5
coarsening_threshold = 0.1
max_level = 5
min_level = 0

# Adaptivity frequency
adapt_frequency = 10

# Budget constraints
max_cell_growth = 2.0
max_cells = 1000000

[neural_error_indicator]
# Features
use_solution_values = true
use_gradient = true
use_hessian = true
use_residual = true
use_geometry = true
neighbor_layers = 2

# Network
hidden_layers = 128,256,128
activation = relu

# Output type
# Options: BINARY, REFINEMENT_LEVEL, CONTINUOUS
output_type = CONTINUOUS

# Training
learning_rate = 1e-3
epochs = 200

[predictive_refinement]
# Prediction horizon
lookahead_steps = 10
lookahead_time = 1.0

# Network
hidden_layers = 256,512,256
use_temporal_encoding = true

# Features
use_velocity_field = true
use_physics_predictors = true

[physics_aware_refinement]
# Physics types to consider
flow_physics = true
geomechanics = true
fracture_physics = true
thermal = false

# Coupling awareness
consider_coupling = true

# Per-physics weights (can be learned)
weight_flow = 1.0
weight_geomechanics = 1.0
weight_fracture = 2.0
weight_thermal = 0.5

# =============================================================================
# Usage Examples
# =============================================================================
# For uncertainty quantification:
#   use_uq = true
#   uq_method = DEEP_ENSEMBLE
#   num_ensemble_members = 5
#
# For history matching:
#   use_neural_inversion = true
#   inversion_method = NEURAL_ENKF
#
# For adaptive time stepping:
#   use_neural_timestep = true
#   neural_imex_enabled = true
#
# For adaptive mesh refinement:
#   use_neural_amr = true
#   amr_use_predictive = true
# =============================================================================
